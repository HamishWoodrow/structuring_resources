{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Number of Documents by Parse Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Parse = title, authors, abstract, references, body\n",
    "\n",
    "Partial Parse = abstract, body, references\n",
    "\n",
    "No Body Parse = no body\n",
    "\n",
    "No Parse = Nothing to little parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = MongoClient().ds_documents\n",
    "col = db.papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs = col.count()\n",
    "full_parse = col.find(filter={'parse_status':'full_parse'}).count(True)\n",
    "partial_parse = col.find(filter={'parse_status':'partial_parse'}).count(True)\n",
    "no_body_parse = col.find(filter={'parse_status':'no_body'}).count(True)\n",
    "no_parse = col.find(filter={'parse_status':'no_parse'}).count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Documents: 128418\n",
      "Full Parse: 98717 (76.87%)\n",
      "Partial Parse: 21152 (16.47%)\n",
      "No Body Parse: 42 (0.03%)\n",
      "No Parse: 8507 (6.62%)\n"
     ]
    }
   ],
   "source": [
    "print('Total Documents: '+str(total_docs))\n",
    "print('Full Parse: '+ str(full_parse) + ' (' + str(round(full_parse/total_docs*100,2))+'%' + ')')\n",
    "print('Partial Parse: '+str(partial_parse)+ ' (' +str(round(partial_parse/total_docs*100,2))+'%' + ')')\n",
    "print('No Body Parse: '+str(no_body_parse)+ ' (' +str(round(no_body_parse/total_docs*100,2))+'%' + ')')\n",
    "print('No Parse: '+str(no_parse)+ ' (' + str(round(no_parse/total_docs*100,2))+'%' + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Parsing and Concept Vocab Generation\n",
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runs the paper processor methods, which take a corpus of documents in from a specific domain in and creates a concept dictionary for all the concepts/subjects which are covered in the texts .  This uses Dbpedia as a knowledge base in order to validate concepts.\n",
    "\n",
    "The script also outputs a TFIDF matrix for the concept terms within the papers.  This is to be later used for creating similarity vectors for the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paper_processor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_words = set(nltk.corpus.stopwords.words('english'))\n",
    "ignored_words.update(('cid','et','e.g.','et al','al', 'yes', 'method',\n",
    "                    'results','citation','use','used','submitted',\n",
    "                    'published', 'professor','dtu', 'pubdb',\n",
    "                    'university','acknowledgements','arxiv',\n",
    "                    'association','society','.',','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Parse and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept_vectorizer class is a modified count vectorizer method based on Sklearn's implementation.  It varies by creating n_grams of the passed documents and uses collocation of these terms in order to search a greater space of possible concepts and subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_parse_cursor = col.find(filter={'parse_status':'full_parse'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Starting first parse ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 95821/98717 [11:15<00:20, 141.87it/s]\n"
     ]
    }
   ],
   "source": [
    "print('##### Starting first parse ####')\n",
    "counter = concept_vectorizer(stop_words=ignored_words,min_df=5,path=True,doc_path_type='db_cursor')\n",
    "path = full_parse_cursor\n",
    "counter_vecs = counter.fit_transform(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('counter_vecs.pickle','wb') as fhand:\n",
    "    pickle.dump(counter_vecs,fhand)\n",
    "with open('counter.pickle','wb') as fh:\n",
    "    pickle.dump(counter,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = pickle.load(open('counter.pickle','rb'))\n",
    "counter_vecs = pickle.load(open('counter_vecs.pickle','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept_validator function takes the concept_vectorizer object and extracts the feature names for the count vectorized n_gram frequency matrix.  \n",
    "It queries each of these n_grams against dbpedia and validates whether the concept exists or not and also returns a list of possible similar terms for the same concept providing disambiguation terms.\n",
    "This results in creating a validated and reduced concept vocabulary for a given corpus of texts, giving the full list of concepts covered in these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('##### Validating the topics on DBPedia ####')\n",
    "concept_dict = concept_validator(counter)\n",
    "vocab_dict = dbpedia_metadata(concept_dict)\n",
    "with open('vocab_dict.json','w') as fh:\n",
    "    json.dump(vocab_dict,fh)\n",
    "print('------ Vocab dict written to json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15723/15723 [00:00<00:00, 86189.74it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('vocab_dict.json','r') as fh:\n",
    "    vocab_dict = json.load(fh)\n",
    "\n",
    "fixed_concept_dict = fixed_concept_dict(vocab_dict)\n",
    "fix_vocab = fixed_concept_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with a fixed domain vocabulary the corpus is reparsed and tokenized but with a fixed search vocabulary for the counts of the concepts covered in each document.\n",
    "The use of disambiguation terms will reference terms which are just synonyms to one fixed term, therefore meaning Machine Learning = ML.\n",
    "\n",
    "After the count vectorized concept matrix a TFIDF transform is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#### Reparsing documents with vocabulary #####')\n",
    "\n",
    "counter_fixed = concept_vectorizer(stop_words=ignored_words,min_df=3,fixed_vocab=fix_vocab,doc_path_type='db_cursor')\n",
    "path = full_parse_cursor\n",
    "counter_vec_fixed = counter_fixed.fit_transform(path)\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_vec = tfidf.fit_transform(counter_vec_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(counter_fixed,open('counter_fixed.p','wb'))\n",
    "pickle.dump(counter_vec_fixed,open('counter_vec_fixed.p','wb'))\n",
    "pickle.dump(tfidf,open('tfidf.p','wb'))\n",
    "pickle.dump(tfidf_vec,open('tfidf_vec.p','wb'))\n",
    "\n",
    "print('#### Finished and models pickled ####')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (struct_ds)",
   "language": "python",
   "name": "structuring_resources-cdnbpurn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
